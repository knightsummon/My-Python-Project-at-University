{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import langdetect \n",
    "import spacy\n",
    "from sklearn import feature_extraction, manifold\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename Columns\n",
    "df = df.rename(columns={\"category\":\"y\", \"headline\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the blank value\n",
    "df.isnull().sum()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the blank value\n",
    "df.isnull().sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Data understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar chart showing label frequencies to study target variable distribution\n",
    "x = \"y\"\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(x, fontsize=12)\n",
    "df[x].reset_index().groupby(x).count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is obvious that the number of travel variables is greater than home&living"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang'] = df[\"text\"].apply(lambda x: langdetect.detect(x) if \n",
    "                                 x.strip() != \"\" else \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"lang\"\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(x, fontsize=12)\n",
    "df[x].reset_index().groupby(x).count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear that English is the primary language, so we filter the dataset in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"lang\"]==\"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Clean the text (convert to lowercase, remove punctuation and characters, then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(lst_text, lst_stopwords):\n",
    "    # Remove stopwords\n",
    "    return [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "def stem_text(lst_text, flg_stemm):\n",
    "    # Perform stemming\n",
    "    if flg_stemm:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        return [ps.stem(word) for word in lst_text]\n",
    "    return lst_text\n",
    "\n",
    "def lemmatize_text(lst_text, flg_lemm):\n",
    "    # Perform lemmatization\n",
    "    if flg_lemm:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        return [lem.lemmatize(word) for word in lst_text]\n",
    "    return lst_text\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = remove_stopwords(lst_text, lst_stopwords)\n",
    "    \n",
    "    # Stem the text\n",
    "    lst_text = stem_text(lst_text, flg_stemm)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lst_text = lemmatize_text(lst_text, flg_lemm)\n",
    "    \n",
    "    # Convert the list back to a string\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: utils_preprocess_text(x, False, True, lst_stopwords))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of commonly used terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"TRAVEL\"\n",
    "top=10\n",
    "corpus = df[df[\"y\"]==y][\"text_clean\"]\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "\n",
    "## unigrams\n",
    "dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "dtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                       columns=[\"Word\",\"Freq\"])\n",
    "dtf_uni.set_index(\"Word\").iloc[:top,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                  legend=False).grid(axis='x')\n",
    "ax[0].set(ylabel=None)\n",
    "\n",
    "## bigrams\n",
    "dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "dtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                      columns=[\"Word\",\"Freq\"])\n",
    "dtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n",
    "                   string for string in x) )\n",
    "dtf_bi.set_index(\"Word\").iloc[:top,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                  legend=False).grid(axis='x')\n",
    "ax[1].set(ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"HOME & LIVING\"\n",
    "top=10\n",
    "corpus = df[df[\"y\"]==y][\"text_clean\"]\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "\n",
    "## unigrams\n",
    "dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "dtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                       columns=[\"Word\",\"Freq\"])\n",
    "dtf_uni.set_index(\"Word\").iloc[:top,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                  legend=False).grid(axis='x')\n",
    "ax[0].set(ylabel=None)\n",
    "\n",
    "## bigrams\n",
    "dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "dtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                      columns=[\"Word\",\"Freq\"])\n",
    "dtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n",
    "                   string for string in x) )\n",
    "dtf_bi.set_index(\"Word\").iloc[:top,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                  legend=False).grid(axis='x')\n",
    "ax[1].set(ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse other features in the dataset and their relationship with the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty values in the 'authors' field with 'Unknown'\n",
    "df['authors'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Calculate the number of articles for each author and plot a bar chart\n",
    "author_counts = df['authors'].value_counts()\n",
    "plt.figure(figsize=(10,5))\n",
    "author_counts[:20].plot(kind='bar')\n",
    "plt.title('Top 20 authors with most articles')\n",
    "plt.show()\n",
    "\n",
    "# Convert the 'date' field to datetime format and extract the month and day of the week\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "\n",
    "# Plot the relationship between month and article categories\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='month', hue='y')\n",
    "plt.title('Article categories distribution by month')\n",
    "plt.show()\n",
    "\n",
    "# Plot the relationship between day of the week and article categories\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='day_of_week', hue='y')\n",
    "plt.title('Article categories distribution by day of the week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_count'] = df[\"text\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "df['word_count'] = df[\"text\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['avg_sentence_lenght'] = df['word_count'] / df['sentence_count']\n",
    "df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_sentence_lenght']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the blank value\n",
    "df.isnull().sum()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,LSTM,Dropout,GlobalAveragePooling1D,Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training data from .CSV file\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df['text_clean'].values\n",
    "y_train = train_df['y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read Testing data From .CSV file\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_test = test_df['text_clean'].values\n",
    "y_test = test_df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Validation data from .CSV\n",
    "val_df = pd.read_csv('valid.csv')\n",
    "\n",
    "# Extract labels and features\n",
    "X_val = val_df['text_clean'].values\n",
    "y_val = val_df['y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =20000\n",
    "max_length = 150\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert training data to sequences\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert training labels to one-hot encoded vectors\n",
    "y_train = np.asarray(y_train)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "\n",
    "# Convert validation data to sequences\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert validation labels to one-hot encoded vectors\n",
    "y_val = np.asarray(y_val)\n",
    "y_val = pd.get_dummies(y_val)\n",
    "\n",
    "# Convert data and labels to arrays\n",
    "train_set = np.array(X_train)\n",
    "val_set = np.array(X_val)\n",
    "train_label = np.array(y_train)\n",
    "val_label = np.array(y_val)\n",
    "# test_set = np.array(X_test)\n",
    "# test_label = np.array(y_test)\n",
    "\n",
    "# Convert test labels to one-hot encoded vectors and extract the class indices\n",
    "y_test = pd.get_dummies(y_test)\n",
    "y_test = np.asarray(y_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print the shapes of the training and validation data and labels\n",
    "print('Train set shape:', train_set.shape)\n",
    "print('Train label shape:', train_label.shape)\n",
    "print('Validation set shape:', val_set.shape)\n",
    "print('Validation label shape:', val_label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "\n",
    "# # Convert the two-dimensional label array to a one-dimensional array\n",
    "train_label_1 = np.argmax(train_label, axis=1)\n",
    "val_label_1 = np.argmax(val_label, axis=1)\n",
    "\n",
    "# # Train the model using the training set\n",
    "# nb_classifier.fit(train_set, train_label_1)\n",
    "\n",
    "# # Make predictions on the training set\n",
    "# train_predictions = nb_classifier.predict(train_set)\n",
    "# train_accuracy = accuracy_score(train_label_1, train_predictions)\n",
    "# print(\"Training Set Accuracy:\", train_accuracy)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# val_predictions = nb_classifier.predict(val_set)\n",
    "# val_accuracy = accuracy_score(val_label_1, val_predictions)\n",
    "# print(\"Validation Set Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print all the parameters of the trained model\n",
    "# print(\"Model Parameters:\")\n",
    "# print(nb_classifier.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# # Save the trained model\n",
    "# joblib.dump(nb_classifier, 'naive_bayes_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a Decision Tree classifier\n",
    "# dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# # Train the model using the training set\n",
    "# dt_classifier.fit(train_set, train_label_1)\n",
    "\n",
    "# # Make predictions on the training set\n",
    "# train_predictions = dt_classifier.predict(train_set)\n",
    "# train_accuracy = accuracy_score(train_label_1, train_predictions)\n",
    "# print(\"Training Set Accuracy:\", train_accuracy)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# val_predictions = dt_classifier.predict(val_set)\n",
    "# val_accuracy = accuracy_score(val_label_1, val_predictions)\n",
    "# print(\"Validation Set Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print all the parameters of the trained model\n",
    "# print(\"Model Parameters:\")\n",
    "# print(dt_classifier.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(dt_classifier, 'dt_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file =  'glove.6B.100d.txt/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index.items()) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Create an empty dictionary to store word vectors\n",
    "embeddings_index = {}\n",
    "\n",
    "# Open the GloVe file and read line by line\n",
    "with open(path_to_glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Split each line into word and coefficients\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        # Convert coefficients to a numpy array\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        # Store the word and its corresponding coefficients in the dictionary\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Print the number of word vectors found in the GloVe file\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "# Create an embedding matrix with all zeros\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "# Iterate over each word in the tokenizer's word index\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # Get the corresponding embedding vector from the embeddings dictionary\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # If an embedding vector is found, update the corresponding row in the embedding matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        # If an embedding vector is not found, keep the row as all zeros\n",
    "        misses += 1\n",
    "\n",
    "# Print the number of words converted to embeddings and the number of misses\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  end-to-end classifier using deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "# embed_size = 100\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     # Embedding layer with pre-trained word embeddings\n",
    "#     Embedding(num_tokens,\n",
    "#               embedding_dim,\n",
    "#               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#               mask_zero=True,\n",
    "#               input_shape=[None],\n",
    "#               trainable=False),\n",
    "#     # Bidirectional LSTM layer\n",
    "#     keras.layers.Bidirectional(keras.layers.LSTM(256, dropout=0.4)),\n",
    "#     # Output Dense layer with softmax activation for binary classification\n",
    "#     keras.layers.Dense(2, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "# history = model.fit( train_set,train_label,\n",
    "#                      batch_size = 32,\n",
    "#                      steps_per_epoch=len(X_train) // 32, \n",
    "#                      validation_data = (val_set , val_label),\n",
    "#                      validation_steps = len(val_set)//32, epochs=20,\n",
    "#                      callbacks=  early_stop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"deeplearning_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "dl_model=load_model(\"deeplearning_model.h5\")\n",
    "# Load the Naive Bayes model from the file\n",
    "naive_bayes_model = joblib.load('naive_bayes_model.pkl')\n",
    "#Load the Decision Tree model from the file\n",
    "decision_tree_model = joblib.load('dt_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I choose accuracy_score as the metric There are several reasons for this:Intuitiveness: Accuracy is an intuitive and easy to understand metric that indicates the percentage of samples correctly predicted by the model. People usually have an intuitive understanding of accuracy and are able to compare it to other metrics.Composite: Accuracy combines the predictive power of a model across all categories, and it takes into account the model's classification accuracy across the entire data set. This makes accuracy a commonly used performance metric, especially when the categories are of similar importance.Generality: Accuracy is applicable to many types of classification problems, whether dichotomous or multicategorical. This makes it a common evaluation metric that can be applied to a variety of domains and application scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The larger the accuracy_score, the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set\n",
    "train_predictions = naive_bayes_model.predict(train_set)\n",
    "train_accuracy = accuracy_score(train_label_1, train_predictions)\n",
    "print(\"Training Set Accuracy:\", train_accuracy)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = naive_bayes_model.predict(val_set)\n",
    "val_accuracy = accuracy_score(val_label_1, val_predictions)\n",
    "print(\"Validation Set Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the decision tree model\n",
    "train_predictions_dt = decision_tree_model.predict(train_set)\n",
    "train_accuracy_dt = accuracy_score(train_label_1, train_predictions_dt)\n",
    "print(\"Training Set Accuracy:\", train_accuracy_dt)\n",
    "\n",
    "# Make predictions on the validation set using the decision tree model\n",
    "val_predictions_dt = decision_tree_model.predict(val_set)\n",
    "val_accuracy_dt = accuracy_score(val_label_1, val_predictions_dt)\n",
    "print(\"Validation Set Accuracy:\", val_accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the deep learning model\n",
    "train_predictions_dl = dl_model.predict(train_set)\n",
    "train_predictions_dl = np.argmax(train_predictions_dl, axis=1)\n",
    "train_accuracy_dl = accuracy_score(np.argmax(train_label, axis=1), train_predictions_dl)\n",
    "print(\"Training Set Accuracy:\", train_accuracy_dl)\n",
    "\n",
    "# Make predictions on the validation set using the deep learning model\n",
    "val_predictions_dl = dl_model.predict(val_set)\n",
    "val_predictions_dl = np.argmax(val_predictions_dl, axis=1)\n",
    "val_accuracy_dl = accuracy_score(np.argmax(val_label, axis=1), val_predictions_dl)\n",
    "print(\"Validation Set Accuracy:\", val_accuracy_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training set generally outperforms the validation set，Model Performance Deep Learning Model > Decision Tree> naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Finding the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb_classifier1 = MultinomialNB()\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {'alpha': [0.05,0.1, 0.5, 1.0],\n",
    "              'fit_prior': [True, False],\n",
    "              'class_prior': [None, [0.2, 0.8], [0.5, 0.5]]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(nb_classifier1, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training set\n",
    "grid_search.fit(train_set, train_label_1)\n",
    "\n",
    "# Get the best parameter values and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save the optimized Naive Bayes model\n",
    "joblib.dump(best_model, 'optimized_naive_bayes_model.pkl')\n",
    "\n",
    "# Make predictions on the training set using the best model\n",
    "train_predictions_opt_bayes = best_model.predict(train_set)\n",
    "train_accuracy_opt_bayes = accuracy_score(train_label_1, train_predictions_opt_bayes)\n",
    "print(\"Training Set Accuracy (optimized):\", train_accuracy_opt_bayes)\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "val_predictions_opt_bayes = best_model.predict(val_set)\n",
    "val_accuracy_opt_bayes = accuracy_score(val_label_1, val_predictions_opt_bayes)\n",
    "print(\"Validation Set Accuracy (optimized):\", val_accuracy_opt_bayes)\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [None, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training set\n",
    "grid_search.fit(train_set, train_label_1)\n",
    "\n",
    "# Get the best parameter values and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save the optimized Decision Tree model\n",
    "joblib.dump(best_model, 'optimized_decision_tree_model.pkl')\n",
    "\n",
    "# Make predictions on the training set using the best model\n",
    "train_predictions_opt_dt = best_model.predict(train_set)\n",
    "train_accuracy_opt_dt = accuracy_score(train_label_1, train_predictions_opt_dt)\n",
    "print(\"Training Set Accuracy (optimized):\", train_accuracy_opt_dt)\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "val_predictions_opt_dt = best_model.predict(val_set)\n",
    "val_accuracy_opt_dt = accuracy_score(val_label_1, val_predictions_opt_dt)\n",
    "print(\"Validation Set Accuracy (optimized):\", val_accuracy_opt_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an additional LSTM layer to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping callback\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001)\n",
    "\n",
    "# # Clear previous TensorFlow session\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# # Define embedding size\n",
    "# embed_size = 100\n",
    "\n",
    "# # Create the optimized model\n",
    "# model_opt = keras.models.Sequential([\n",
    "#     # Embedding layer with pre-trained word embeddings\n",
    "#     Embedding(num_tokens,\n",
    "#               embedding_dim,\n",
    "#               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#               mask_zero=True,\n",
    "#               input_shape=[None],\n",
    "#               trainable=False),\n",
    "# # Bidirectional LSTM layers\n",
    "#     keras.layers.Bidirectional(keras.layers.LSTM(256, dropout=0.4, return_sequences=True)),\n",
    "#     keras.layers.Bidirectional(keras.layers.LSTM(128, dropout=0.3)),\n",
    "#     keras.layers.Dense(2, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# # Compile the model with the optimizer and loss function\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model_opt.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# # Print the summary of the optimized model\n",
    "# model_opt.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_opt.fit( train_set,train_label,\n",
    "#                      batch_size = 32,\n",
    "#                      steps_per_epoch=len(X_train) // 32, \n",
    "#                      validation_data = (val_set , val_label),\n",
    "#                      validation_steps = len(val_set)//32, epochs=20,\n",
    "#                      callbacks=  early_stop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_opt.save(\"deeplearning_model_opt.h5\")\n",
    "model_opt=load_model(\"deeplearning_model_opt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the deep learning model\n",
    "train_predictions_dl_opt = model_opt.predict(train_set)\n",
    "train_predictions_dl_opt = np.argmax(train_predictions_dl_opt, axis=1)\n",
    "train_accuracy_dl_opt = accuracy_score(np.argmax(train_label, axis=1), train_predictions_dl_opt)\n",
    "print(\"Training Set Accuracy:\", train_accuracy_dl_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set using the deep learning model\n",
    "val_predictions_dl_opt = model_opt.predict(val_set)\n",
    "val_predictions_dl_opt = np.argmax(val_predictions_dl_opt, axis=1)\n",
    "val_accuracy_dl_opt = accuracy_score(np.argmax(val_label, axis=1), val_predictions_dl_opt)\n",
    "print(\"Validation Set Accuracy:\", val_accuracy_dl_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be seen that the classification accuracy of the best model among the three models, i.e., the deep learning model's Accuracy rate of over 90%, which can accomplish the task very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized Naive Bayes model\n",
    "optimized_naive_bayes_model = joblib.load('optimized_naive_bayes_model.pkl')\n",
    "# Combine the training and validation sets\n",
    "combined_train_set = np.concatenate((train_set, val_set), axis=0)\n",
    "combined_train_label = np.concatenate((train_label_1, val_label_1), axis=0)\n",
    "# Perform cross-validation on the combined dataset\n",
    "scores = cross_val_score(optimized_naive_bayes_model, combined_train_set, combined_train_label, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized Naive Bayes model\n",
    "optimized_decision_tree_model = joblib.load('optimized_decision_tree_model.pkl')\n",
    "# Perform cross-validation on the combined dataset\n",
    "scores_dt = cross_val_score(optimized_decision_tree_model, combined_train_set, combined_train_label, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", scores_dt)\n",
    "print(\"Average Accuracy:\", scores_dt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model_opt=load_model(\"deeplearning_model_opt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = np.concatenate((train_set, val_set), axis=0)\n",
    "merged_labels = np.concatenate((train_label, val_label), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Initialize a list to store the evaluation scores for each fold\n",
    "accuracy_scores = []\n",
    "\n",
    "# Train and evaluate the model for each fold\n",
    "for train_index, test_index in kf.split(merged_data):\n",
    "    # Get the training and testing data for the current fold\n",
    "    X_train_fold, X_test_fold = merged_data[train_index], merged_data[test_index]\n",
    "    y_train_fold, y_test_fold = merged_labels[train_index], merged_labels[test_index]\n",
    "\n",
    "    # Use the pre-trained model to predict on the testing fold\n",
    "    y_pred_fold = dl_model_opt.predict(X_test_fold)\n",
    "    y_pred_fold = np.argmax(y_pred_fold, axis=1)\n",
    "\n",
    "    # Calculate the accuracy score for the current fold\n",
    "    accuracy_fold = accuracy_score(np.argmax(y_test_fold, axis=1), y_pred_fold)\n",
    "\n",
    "    # Append the accuracy score to the list\n",
    "    accuracy_scores.append(accuracy_fold)\n",
    "\n",
    "# Print the evaluation scores for each fold\n",
    "for i, score in enumerate(accuracy_scores):\n",
    "    print(\"Fold\", i+1, \"Accuracy:\", score)\n",
    "\n",
    "# Calculate the average accuracy score\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=load_model(\"deeplearning_model_opt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to sequences\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert test labels to one-hot encoded vectors\n",
    "y_test = np.asarray(y_test)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "# Convert data and labels to arrays\n",
    "test_set = np.array(X_test)\n",
    "test_label = np.array(y_test)\n",
    "# Make predictions on the test set using the deep learning model\n",
    "test_predictions = best_model.predict(test_set)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "test_accuracy = accuracy_score(np.argmax(test_label, axis=1), test_predictions)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicators do not differ significantly from the validation set\n",
    "\n",
    "### Retraining bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the training and validation sets\n",
    "# merged_set = np.concatenate((train_set, val_set), axis=0)\n",
    "# merged_label = np.concatenate((train_label, val_label), axis=0)\n",
    "\n",
    "# # Compile the model with the optimizer and loss function\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "# best_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# # Print the summary of the best model\n",
    "# best_model.summary()\n",
    "\n",
    "# # Train the model on the merged set\n",
    "# history = best_model.fit(merged_set, merged_label,\n",
    "#                          batch_size=32,\n",
    "#                          steps_per_epoch=len(merged_set) // 32,\n",
    "#                          validation_data=(val_set, val_label),\n",
    "#                          validation_steps=len(val_set) // 32,\n",
    "#                          epochs=20,\n",
    "#                          callbacks=early_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_all=load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set using the deep learning model\n",
    "test_predictions = best_model_all.predict(test_set)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "test_accuracy = accuracy_score(np.argmax(test_label, axis=1), test_predictions)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
